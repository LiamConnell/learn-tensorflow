{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "import pandas.io.data as web\n",
    "from pykalman import KalmanFilter\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prices(symbol):\n",
    "    start, end = '2007-05-02', '2016-04-11'\n",
    "    data = web.DataReader(symbol, 'yahoo', start, end)\n",
    "    data=pd.DataFrame(data)\n",
    "    prices=data['Adj Close']\n",
    "    #prices=np.asarray(list(prices))\n",
    "    prices=prices.astype(float)\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results2(prices):\n",
    "    kf=KalmanFilter(transition_matrices=[[1,1],[0,1]],observation_matrices=[[1,0]],\n",
    "                    initial_state_mean=[prices[0],0],transition_covariance=[[.001,0],[0,.001]],\n",
    "                    observation_covariance=1) #TC -> Q ;; OC -> R\n",
    "    # kf=kf.em(prices,n_iter=5)\n",
    "    (a,b)=kf.filter(prices)\n",
    "    test_data = pd.DataFrame(prices)\n",
    "    test_data.columns = ['prices']\n",
    "    test_data['signal'] = test_data\n",
    "    test_data['filter'] = a[:,0]\n",
    "    test_data['resids'] = test_data.prices - test_data['filter']\n",
    "    test_data['position'] = 0\n",
    "    test_data['resid_dev'] = pd.rolling_apply(test_data.resids, 50,  np.std)    # I calculate the threshold on a rolling basis to avoid lookahead bias \n",
    "    test_data.position[test_data.resids<-test_data.resid_dev] = 1\n",
    "    test_data.position[test_data.resids>test_data.resid_dev] = -1\n",
    "    test_data['daily_diff'] = (test_data.signal.shift(-1) - test_data.signal)/test_data.signal\n",
    "    test_data['results'] = test_data.position * test_data.daily_diff\n",
    "    return test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liamconnell/anaconda/lib/python3.4/site-packages/IPython/kernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/liamconnell/anaconda/lib/python3.4/site-packages/IPython/kernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "gs = get_prices('GS')\n",
    "df = get_results2(gs.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = pd.DataFrame(index = df.index)\n",
    "classes['low'] = df.daily_diff <-.1\n",
    "classes['l'] = (df.daily_diff> -.1) & (df.daily_diff <0)\n",
    "classes['h'] = (df.daily_diff> 0) & (df.daily_diff <.1)\n",
    "classes['high'] = (df.daily_diff> .1) #& (df.daily_diff <.1)\n",
    "\n",
    "classes = classes*2/2\n",
    "classes = classes[:-2]\n",
    "#classes = classes[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xdata = np.copy(classes)\n",
    "ydata = np.copy(classes)\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1] = xdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches = np.split(xdata.reshape(batch_size, -1), 45, 1) #45 in num batches MAGIC NUMBER\n",
    "y_batches = np.split(ydata.reshape(batch_size, -1), 45, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(x_batches, y_batches, pointer):\n",
    "    x, y = x_batches[pointer], y_batches[pointer]\n",
    "    pointer += 1\n",
    "    return x, y, pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pointer=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, pointer = next_batch(x_batches, y_batches, pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, infer=True):\n",
    "        if infer:\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "\n",
    "       \n",
    "        cell_fn = rnn_cell.BasicLSTMCell\n",
    "        \n",
    "\n",
    "        cell = cell_fn(args.rnn_size)\n",
    "\n",
    "        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        outputs, last_state = seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([args.batch_size * args.seq_length])],\n",
    "                args.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = self.cell.zero_state(1, tf.float32).eval()\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-637199a1af15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-5ab3e118e0e7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, infer)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    # restore model\n",
    "    \n",
    "    for e in range(args.num_epochs):\n",
    "        sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = model.initial_state.eval()\n",
    "        for b in range(data_loader.num_batches):\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b,\n",
    "                        args.num_epochs * data_loader.num_batches,\n",
    "                        e, train_loss, end - start))\n",
    "            if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                or (e==args.num_epochs-1 and b == data_loader.num_batches-1): # save for the last result\n",
    "                checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
